---
layout: post
title: "Statistics learning naive Bayes"
description: ""
category: 
tags: [Statistics]
---
##Intoduction##
In simple terms, a naive Bayes classifier assumes that the presence or absence of a particular feature is unrelated to the presence or absence of any other feature, given the class variable.      
Given input set X, output set Y, X and Y are both random variables, `\( P(X,Y) \)` denote the joint density function.     
`\( T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})\} \)` is generated by `\( P(X,Y) \)` independently.      
`\( y_{i} \in \{ c_{1},c_{2},...,c_{K} \} \)` and the algorithm goes like this:     
(1). Calculate prior probability and the conditional probability      
`\[ P(Y=c_{k})=\frac{\sum_{i=1}^{N} I(y_{i}=c_{k})}{N} \]`
`\[ P(X^{(j)}=a_{ji}|Y=c_{k})=\frac{\sum_{i=1}^N I(x_{i}^{(j)}=a_{ji},y_{i}=c_{k})}{\sum_{i=1}^N I(y_{i}=c_{k})} \]`
(2).With regard to the given `\( x=(x^{(1)},x^{(2)},...,x^{(n)}) \)`Calculate      
`\[ P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|y_{i}=c_{k}) \]`
(3).Find the class where x belongs to     
`\[ y=argmax_{c_{k}}P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|y_{i}=c_{k}) \]`

